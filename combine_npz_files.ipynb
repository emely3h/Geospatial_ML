{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNiF0UTKEIu6zzw7ZvqNYuE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emely3h/Geospatial_ML/blob/feature%2Fadd-data-generators-to-fix-ram-problem/combine_npz_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine npz files\n",
        "\n",
        "This is a notebook for the last step in the prepare data pipeline as we did not have enough RAM to run it locally. To train the model on the entire dataset it is more convenient to have all tile-arrays of all images in one .npz file. "
      ],
      "metadata": {
        "id": "oUgJx7pfMJjO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "m8S-npIwb0bv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2593bee-5e5d-49fd-9d13-eea7558fc3a3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/15HUD3sGdfvxy5Y_bjvuXgrzwxt7TzRfm/MachineLearning\n",
            "/content/drive/.shortcut-targets-by-id/15HUD3sGdfvxy5Y_bjvuXgrzwxt7TzRfm/MachineLearning/Geospatial_ML\n",
            "architecture.drawio  colab.py\t       experiments   __pycache__\n",
            "colab-new.py\t     data_exploration  models\t     README.md\n",
            "colab_new.py\t     evaluation        prepare_data  requirements.txt\n"
          ]
        }
      ],
      "source": [
        "#! ls\n",
        "%cd drive/MyDrive/MachineLearning\n",
        "%cd Geospatial_ML\n",
        "! ls"
      ],
      "metadata": {
        "id": "K4ovYlrBb0bx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43df6ab6-7b61-4e78-cbaa-0a01921ae0b0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import datetime\n",
        "data_path = \"../data_colab/256_200\""
      ],
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "m596oOw2b0bz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "uncompressed file is 2GB, 50MB and compressed 274 MB\n",
        "=> loading/ decompressing all arrays takes ~ 15 * 2,04 GB = 31 GB\n",
        "=> loading all images into RAM still works but compressing them fails\n",
        "\n",
        "all 5 images decompressed in memory ~ 18 GB RAM\n",
        "\n",
        "- combining 8 images with savez() takes 20 GB < 5min, < 20GB System RAM\n",
        "- combining 8 images with savez_compressed() takes 1,84 GB > 10min, ~ 30 GB System RAM\n",
        "- trying to combine 11 images with savez_compressed crashed during loading 9th image\n"
      ],
      "metadata": {
        "id": "j1ris5P21Pvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem: running out of ram when trying to save more than 5 images in one compressed npz, crashing always just at the savez_compressed() step\n",
        "# => combining only 5 images into one file and then trying to combine those 2 files if possible\n",
        "# => better way? Why does savez_compressed() consume most RAM?\n",
        "# 50GB not enough for saving 10 images => loading + decompressing images takes ~ 30 GB why does last step, saving take so much RAM?\n",
        "def combine_npz_arrays(data_path):\n",
        "    count = 0\n",
        "    arrays_dict = {}\n",
        "    print(f'Started at: {datetime.datetime.now()}')\n",
        "    for file in os.listdir(data_path):\n",
        "        if file != '2022_08_09.npz' and count < 9: # Todo: find out problem with image 2022_08_09 => crashes when trying to access x_input\n",
        "            print(f'Adding image {file}')\n",
        "            array = np.load(f'{data_path}/{file}')\n",
        "            x_input = array['x_input']\n",
        "            y_mask = array['y_mask']\n",
        "            if len(arrays_dict) < 1:\n",
        "                arrays_dict['x_input'] = x_input\n",
        "                arrays_dict['y_mask'] = y_mask\n",
        "            else:\n",
        "                arrays_dict['x_input'] = np.concatenate((arrays_dict['x_input'], x_input), axis=0)\n",
        "                arrays_dict['y_mask'] = np.concatenate((arrays_dict['y_mask'], y_mask), axis=0)\n",
        "        print(arrays_dict['x_input'].shape)\n",
        "        print(arrays_dict['y_mask'].shape)\n",
        "        print()\n",
        "        count += 1\n",
        "    # test time to execute and file size of np.savez and np.savez_compressed\n",
        "    np.savez_compressed(f'{data_path}/all_images_1', **arrays_dict) #savez_compressed\n",
        "    print('Combined all compressed numpy images into one single file.')\n",
        "    print(f'Finished at: {datetime.datetime.now()}')\n",
        "\n",
        "combine_npz_arrays(\"../data_colab/256_200\")"
      ],
      "metadata": {
        "id": "P2t8Rh1eccZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4216f5-73af-4ad8-94fb-d1453779fa21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started at: 2023-03-28 17:01:23.368041\n",
            "Adding image 2022_10_13.npz\n",
            "(889, 256, 256, 5)\n",
            "(889, 256, 256)\n",
            "\n",
            "Adding image 2022_07_15.npz\n",
            "(1753, 256, 256, 5)\n",
            "(1753, 256, 256)\n",
            "\n",
            "Adding image 2022_09_18.npz\n",
            "(2927, 256, 256, 5)\n",
            "(2927, 256, 256)\n",
            "\n",
            "Adding image 2022_06_20.npz\n",
            "(4178, 256, 256, 5)\n",
            "(4178, 256, 256)\n",
            "\n",
            "Adding image 2022_10_23.npz\n",
            "(5342, 256, 256, 5)\n",
            "(5342, 256, 256)\n",
            "\n",
            "Adding image 2022_07_25.npz\n",
            "(6600, 256, 256, 5)\n",
            "(6600, 256, 256)\n",
            "\n",
            "Adding image 2022_08_04.npz\n",
            "(7919, 256, 256, 5)\n",
            "(7919, 256, 256)\n",
            "\n",
            "Adding image 2022_07_10.npz\n",
            "(9242, 256, 256, 5)\n",
            "(9242, 256, 256)\n",
            "\n",
            "Adding image 2022_07_30.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array = np.load(f'{data_path}/2022_06_20.npz')\n",
        "x_input = array['x_input']\n",
        "y_mask = array['y_mask']\n",
        "print(x_input.shape)\n",
        "print(y_mask.shape)\n",
        "print(array.files)"
      ],
      "metadata": {
        "id": "Au1zMrYWBUCP",
        "outputId": "5cca0294-d701-4e00-8bfe-690ed711652b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1251, 256, 256, 5)\n",
            "(1251, 256, 256)\n",
            "['y_mask', 'x_input']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"../data_colab/256_200\"\n",
        "\n",
        "total_tiles = 0\n",
        "for file in os.listdir(data_path):\n",
        "  if not os.path.isdir(os.path.join(data_path, file)):\n",
        "   \n",
        "    print(f'Image: {file}')\n",
        "    array = np.load(f'{data_path}/{file}')\n",
        "    total_tiles += array['x_input'].shape[0]\n",
        "    print(array['x_input'].shape)\n",
        "    print(array['y_mask'].shape)\n",
        "    print()\n",
        "\n",
        "print(f'Total amount of tiles {total_tiles}')"
      ],
      "metadata": {
        "id": "YW0PJi8Qs6xu",
        "outputId": "70daaea7-d8ea-48ef-973c-b4836c3fd430",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 2022_10_13.npz\n",
            "(889, 256, 256, 5)\n",
            "(889, 256, 256)\n",
            "\n",
            "Image: 2022_07_15.npz\n",
            "(864, 256, 256, 5)\n",
            "(864, 256, 256)\n",
            "\n",
            "Image: 2022_09_18.npz\n",
            "(1174, 256, 256, 5)\n",
            "(1174, 256, 256)\n",
            "\n",
            "Image: 2022_06_20.npz\n",
            "(1251, 256, 256, 5)\n",
            "(1251, 256, 256)\n",
            "\n",
            "Image: 2022_10_23.npz\n",
            "(1164, 256, 256, 5)\n",
            "(1164, 256, 256)\n",
            "\n",
            "Image: 2022_07_25.npz\n",
            "(1258, 256, 256, 5)\n",
            "(1258, 256, 256)\n",
            "\n",
            "Image: 2022_08_04.npz\n",
            "(1319, 256, 256, 5)\n",
            "(1319, 256, 256)\n",
            "\n",
            "Image: 2022_07_10.npz\n",
            "(1323, 256, 256, 5)\n",
            "(1323, 256, 256)\n",
            "\n",
            "Image: 2022_07_30.npz\n",
            "(1183, 256, 256, 5)\n",
            "(1183, 256, 256)\n",
            "\n",
            "Image: 2022_08_14.npz\n",
            "(1179, 256, 256, 5)\n",
            "(1179, 256, 256)\n",
            "\n",
            "Image: 2022_08_24.npz\n",
            "(1306, 256, 256, 5)\n",
            "(1306, 256, 256)\n",
            "\n",
            "Image: 2022_09_03.npz\n",
            "(1196, 256, 256, 5)\n",
            "(1196, 256, 256)\n",
            "\n",
            "Image: 2022_12_12.npz\n",
            "(957, 256, 256, 5)\n",
            "(957, 256, 256)\n",
            "\n",
            "Image: 2022_09_08.npz\n",
            "(927, 256, 256, 5)\n",
            "(927, 256, 256)\n",
            "\n",
            "Image: 2022_12_02.npz\n",
            "(1142, 256, 256, 5)\n",
            "(1142, 256, 256)\n",
            "\n",
            "Image: 2022_09_13.npz\n",
            "(1175, 256, 256, 5)\n",
            "(1175, 256, 256)\n",
            "\n",
            "Image: 2022_08_09.npz\n",
            "(1181, 256, 256, 5)\n",
            "(1181, 256, 256)\n",
            "\n",
            "Total amount of tiles 19488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the shape of the output array\n",
        "output_shape = (2574, 256, 256)\n",
        "\n",
        "# Create a memory-mapped array to hold the output data\n",
        "output_file = np.memmap(os.path.join(data_path, \"combined_y_mask.npy\"), mode=\"w+\", shape=output_shape, dtype=np.float32)\n",
        "file_count = 0\n",
        "# Iterate over each compressed numpy array\n",
        "for file in os.listdir(data_path):\n",
        "  if not os.path.isdir(os.path.join(data_path, file)) and not file.startswith('combined') and not file.startswith('compressed'):\n",
        "    file_count += 1\n",
        "    print(f'loading file {file_count}: {file}')\n",
        "    # Load the compressed numpy array in chunks using np.memmap\n",
        "    with np.load(os.path.join(data_path, file), mmap_mode=\"r\") as data:\n",
        "        chunk_size = 50  # Number of samples to load per chunk\n",
        "        num_chunks = data[\"y_mask\"].shape[0] // chunk_size\n",
        "        for j in range(num_chunks):\n",
        "            print(f'Chunk {j}')\n",
        "            start_idx = (file_count - 1) * num_chunks * chunk_size + j * chunk_size\n",
        "            end_idx = start_idx + chunk_size\n",
        "            # Write the chunk to the output file using the memory-mapped array\n",
        "            print(f'output file indexes: {start_idx} : {end_idx}  Chunk shape {data[\"y_mask\"][j * chunk_size:(j + 1) * chunk_size, ...].shape}')\n",
        "            output_file[start_idx:end_idx, ...] = data[\"y_mask\"][j * chunk_size:(j + 1) * chunk_size, ...]\n",
        "print('finished concatenating arrays')\n",
        "output_file.flush()\n",
        "print('finished flushing')\n",
        "\n",
        "# Problem: file not saved in drive...? => takes time to sync"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irt8ij7yquS5",
        "outputId": "1ddfef0e-d8cb-41cf-df28-5a53d4a4d167"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading file 1: 2022_10_13.npz\n",
            "Chunk 0\n",
            "output file indexes: 0 : 50  Chunk shape (50, 256, 256)\n",
            "Chunk 1\n",
            "output file indexes: 50 : 100  Chunk shape (50, 256, 256)\n",
            "Chunk 2\n",
            "output file indexes: 100 : 150  Chunk shape (50, 256, 256)\n",
            "Chunk 3\n",
            "output file indexes: 150 : 200  Chunk shape (50, 256, 256)\n",
            "Chunk 4\n",
            "output file indexes: 200 : 250  Chunk shape (50, 256, 256)\n",
            "Chunk 5\n",
            "output file indexes: 250 : 300  Chunk shape (50, 256, 256)\n",
            "Chunk 6\n",
            "output file indexes: 300 : 350  Chunk shape (50, 256, 256)\n",
            "Chunk 7\n",
            "output file indexes: 350 : 400  Chunk shape (50, 256, 256)\n",
            "Chunk 8\n",
            "output file indexes: 400 : 450  Chunk shape (50, 256, 256)\n",
            "Chunk 9\n",
            "output file indexes: 450 : 500  Chunk shape (50, 256, 256)\n",
            "Chunk 10\n",
            "output file indexes: 500 : 550  Chunk shape (50, 256, 256)\n",
            "Chunk 11\n",
            "output file indexes: 550 : 600  Chunk shape (50, 256, 256)\n",
            "Chunk 12\n",
            "output file indexes: 600 : 650  Chunk shape (50, 256, 256)\n",
            "Chunk 13\n",
            "output file indexes: 650 : 700  Chunk shape (50, 256, 256)\n",
            "Chunk 14\n",
            "output file indexes: 700 : 750  Chunk shape (50, 256, 256)\n",
            "Chunk 15\n",
            "output file indexes: 750 : 800  Chunk shape (50, 256, 256)\n",
            "Chunk 16\n",
            "output file indexes: 800 : 850  Chunk shape (50, 256, 256)\n",
            "loading file 2: 2022_07_15.npz\n",
            "Chunk 0\n",
            "output file indexes: 850 : 900  Chunk shape (50, 256, 256)\n",
            "Chunk 1\n",
            "output file indexes: 900 : 950  Chunk shape (50, 256, 256)\n",
            "Chunk 2\n",
            "output file indexes: 950 : 1000  Chunk shape (50, 256, 256)\n",
            "Chunk 3\n",
            "output file indexes: 1000 : 1050  Chunk shape (50, 256, 256)\n",
            "Chunk 4\n",
            "output file indexes: 1050 : 1100  Chunk shape (50, 256, 256)\n",
            "Chunk 5\n",
            "output file indexes: 1100 : 1150  Chunk shape (50, 256, 256)\n",
            "Chunk 6\n",
            "output file indexes: 1150 : 1200  Chunk shape (50, 256, 256)\n",
            "Chunk 7\n",
            "output file indexes: 1200 : 1250  Chunk shape (50, 256, 256)\n",
            "Chunk 8\n",
            "output file indexes: 1250 : 1300  Chunk shape (50, 256, 256)\n",
            "Chunk 9\n",
            "output file indexes: 1300 : 1350  Chunk shape (50, 256, 256)\n",
            "Chunk 10\n",
            "output file indexes: 1350 : 1400  Chunk shape (50, 256, 256)\n",
            "Chunk 11\n",
            "output file indexes: 1400 : 1450  Chunk shape (50, 256, 256)\n",
            "Chunk 12\n",
            "output file indexes: 1450 : 1500  Chunk shape (50, 256, 256)\n",
            "Chunk 13\n",
            "output file indexes: 1500 : 1550  Chunk shape (50, 256, 256)\n",
            "Chunk 14\n",
            "output file indexes: 1550 : 1600  Chunk shape (50, 256, 256)\n",
            "Chunk 15\n",
            "output file indexes: 1600 : 1650  Chunk shape (50, 256, 256)\n",
            "Chunk 16\n",
            "output file indexes: 1650 : 1700  Chunk shape (50, 256, 256)\n",
            "finished concatenating arrays\n",
            "finished flushing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez_compressed(os.path.join(data_path, \"compressed_combined_y_mask.npz\"), x_input=output_file)\n",
        "\n",
        "# Delete the memory-mapped array to free up resources\n",
        "del output_file"
      ],
      "metadata": {
        "id": "t8hu82SD9ftp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array = np.load(f'{data_path}/compressed_combined_x_input.npz', allow_pickle=True)\n",
        "x_input = array['x_input']\n",
        "print(array.files)\n",
        "#y_mask = array['y_mask']\n",
        "print(x_input.shape)\n",
        "#print(y_mask.shape)"
      ],
      "metadata": {
        "id": "NQGSglKAnmLQ",
        "outputId": "1406d796-6d41-40c0-e791-3438657122b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['x_input']\n",
            "(19488, 256, 256, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array = np.load(f'{data_path}/combined_x_input.npy')\n",
        "x_input = array['x_input']\n",
        "y_mask = array['y_mask']\n",
        "print(x_input.shape)\n",
        "print(y_mask.shape)"
      ],
      "metadata": {
        "id": "gboBDRWJKwDD",
        "outputId": "dd5bde3b-e870-4aef-d8f9-e4ae04b4f905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a291d656a036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{data_path}/combined_x_input.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[1;32m    436\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array_y.files\n",
        "array_y['x_input'].shape"
      ],
      "metadata": {
        "id": "qiVoiO5ENaDt",
        "outputId": "711c84d5-02a5-4f0b-b6b0-02e4d70cf30f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2574, 256, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "time to execute npy: 50 min\n",
        "\n",
        "system ram needed: ~10 GB\n",
        "\n",
        "crashed on last img 2022_08_09: ValueError                                Traceback (most recent call last)\n",
        "\n",
        "<ipython-input-19-2ced60a8ab90> in <module>\n",
        "     19             end_idx = start_idx + chunk_size\n",
        "     20             # Write the chunk to the output file using the memory-mapped array\n",
        "---> 21             output_file[start_idx:end_idx, ...] = data[\"x_input\"][j * chunk_size:(j + 1) * chunk_size, ...]\n",
        "     22 \n",
        "     23 # Delete the memory-mapped array to free up resources\n",
        "\n",
        "ValueError: could not broadcast input array from shape (50,256,256,5) into shape (38,256,256,5)\n",
        "\n",
        "\n",
        "\n",
        "file size npz\n",
        "time to execute npz"
      ],
      "metadata": {
        "id": "JGwuVMgb0iRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> saved as npy can not be read but when npy is then saved as compressed npz readin/ loading works, takes just more time but files are smaller"
      ],
      "metadata": {
        "id": "-uWu40EZLGO1"
      }
    }
  ]
}